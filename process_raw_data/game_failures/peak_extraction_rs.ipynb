{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create main Timeseries object\n",
    "The following cell combines all the parsed csv's (one per month) from the given folder into a single Timeseries object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "register_matplotlib_converters()\n",
    "\n",
    "\n",
    "# ------ Path containing CSVs to be parsed and merged ------ #\n",
    "PATH = \"runescape\"\n",
    "# ---------------------------------------------------------- #\n",
    "\n",
    "# ------ Path to the manually labeled file ----------------- #\n",
    "REAL_LABELS_FILE = \"minecraft_new_hypixel/labeled/hypixel_all_labeled.csv\"\n",
    "# ---------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# ------ Resampling frequency ------------------------------ #\n",
    "RESAMPLE_FREQ = 2 # MINUTES. New sample every X minutes.\n",
    "# ---------------------------------------------------------- #\n",
    "\n",
    "\n",
    "all_files = glob.glob(PATH + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "for filename in all_files:\n",
    "    ts = pd.read_csv(filename, header=0, parse_dates=[0], index_col=0, squeeze=True)\n",
    "    li.append(ts)\n",
    "\n",
    "final_ts = pd.concat(li, axis=0, ignore_index=False)\n",
    "final_ts = final_ts.sort_index()\n",
    "\n",
    "# Resample using globally defined freq\n",
    "final_ts = final_ts.resample(str(RESAMPLE_FREQ)+\"min\", level=0).mean()\n",
    "\n",
    "#Non-interpolated dataset:\n",
    "raw_ts = final_ts\n",
    "\n",
    "# Interpolated final Timeseries:\n",
    "final_ts = final_ts.interpolate()\n",
    "\n",
    "\n",
    "# ------------------ OPTIONAL: Truncate dataset to focus on specific period ------------------ #\n",
    "\n",
    "#final_ts = final_ts.truncate(pd.Timestamp(2007, 10, 1), pd.Timestamp(2015, 3, 22)) # All data\n",
    "#final_ts = final_ts.truncate(pd.Timestamp(2014, 7, 1), pd.Timestamp(2014, 7, 30))  # Test month\n",
    "final_ts = final_ts.truncate(pd.Timestamp(2013, 5, 1), pd.Timestamp(2013, 7, 30))  # Test few months\n",
    "\n",
    "# -------------------------------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmap of player counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660173cad9fa4aeebb10b90d8a49c67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import datetime\n",
    "import calendar\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "df = pd.DataFrame(final_ts)\n",
    "df.insert(1, 'weekday', df.index.weekday)\n",
    "df.insert(2, 'time', df.index.to_series().apply(lambda x: x.replace(day=1, month=1, year=2020)))\n",
    "\n",
    "lst = []\n",
    "\n",
    "hours = range(0,24)\n",
    "weekdays = list(reversed(calendar.day_name))\n",
    "\n",
    "for wd in range(6, -1, -1):\n",
    "    ls = []\n",
    "    for tm in range(0,24):\n",
    "        ls.append(df['0'].where(np.logical_and(df['time'].dt.hour == tm, df['weekday'] == wd)).sum())\n",
    "    lst.append(ls)\n",
    "\n",
    "arr = np.array(lst)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(arr, interpolation=\"nearest\")\n",
    "\n",
    "ax.set_xticks(np.arange(len(hours)))\n",
    "ax.set_yticks(np.arange(len(weekdays)))\n",
    "\n",
    "ax.set_xticklabels(hours)\n",
    "ax.set_yticklabels(weekdays)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "ax.set_title(\"Heatmap of total player count by hour\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Seasonality and Trend from the time series\n",
    "This cell will decompose the time series into the trend and seasonal components. These are then removed from the original dataset to be left with only the residuals.\n",
    "Two separate methods are used. The very fast but less thorough 'seasonal_decompose', and the slow but good STL method.\n",
    "The first method does two passes, to remove both daily and weekly seasonality. This is impractical for the STL method due to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df12f32fb13a4a398e15cf8cfaca15e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "SAMPLES_PER_DAY = int((24*60) / RESAMPLE_FREQ) # Amount of samples per day is: minutes per day / sample period in minutes.\n",
    "\n",
    "\n",
    "# ------------ Time Series Decomposition using 'seasonal_decompose' ------------\n",
    "result_decomp = seasonal_decompose(final_ts, period=SAMPLES_PER_DAY, model=\"additive\")\n",
    "detrended1 = final_ts.values - result_decomp.trend # Detrend\n",
    "deseasonalized1 = detrended1.values - result_decomp.seasonal # Deseasonalize\n",
    "# Second pass for weekly seasonality\n",
    "result_decomp2 = seasonal_decompose(deseasonalized1.fillna(0), period=SAMPLES_PER_DAY*7, model=\"additive\")\n",
    "detrended2 = deseasonalized1.values - result_decomp2.trend\n",
    "deseasonalized_decompose = detrended2.values - result_decomp2.seasonal\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ------------ Time Series Decomposition using 'STL' ---------------------------\n",
    "result_stl = STL(final_ts, period=SAMPLES_PER_DAY).fit()\n",
    "detrended = final_ts.values - result_stl.trend # Detrend\n",
    "deseasonalized_stl = detrended.values - result_stl.seasonal # Deseasonalize\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# --- CHOOSE which result to use from here on out --- #\n",
    "deseasonalized = deseasonalized_stl\n",
    "# --------------------------------------------------- #\n",
    "\n",
    "\n",
    "# --- Plot the results ---\n",
    "plt.subplot(321)\n",
    "plt.plot(result_stl.trend)\n",
    "ax = plt.gca()\n",
    "ax.legend(['Trend'])\n",
    "plt.subplot(323)\n",
    "plt.plot(result_stl.seasonal)\n",
    "ax = plt.gca()\n",
    "ax.legend(['Seasonality'])\n",
    "plt.subplot(325)\n",
    "plt.plot(result_stl.resid)\n",
    "ax = plt.gca()\n",
    "ax.legend(['Residuals'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(deseasonalized_decompose, linewidth=1, color='#E14932')\n",
    "plt.plot(deseasonalized_stl, linewidth=1, color=\"orange\")\n",
    "plt.plot(final_ts, color=\"blue\")\n",
    "ax = plt.gca()\n",
    "ax.legend(['seasonal_decompose', 'STL', 'Raw Data'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f12fe08eaa46a7b2cc5ce2b4c90ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "# --- Plot the results ---\n",
    "plt.subplot(321)\n",
    "plt.plot(result_stl.trend)\n",
    "ax = plt.gca()\n",
    "ax.legend(['Trend'])\n",
    "plt.subplot(323)\n",
    "plt.plot(result_stl.seasonal, linewidth=0.1)\n",
    "ax = plt.gca()\n",
    "ax.legend(['Seasonality'])\n",
    "plt.subplot(325)\n",
    "plt.plot(result_stl.resid, linewidth=0.5)\n",
    "ax = plt.gca()\n",
    "ax.legend(['Residuals'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(deseasonalized_decompose, linewidth=1, color='#E14932')\n",
    "plt.plot(deseasonalized_stl, linewidth=1, color=\"orange\")\n",
    "plt.plot(final_ts, color=\"blue\")\n",
    "ax = plt.gca()\n",
    "ax.legend(['seasonal_decompose', 'STL', 'Raw Data'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Failures (using Z-scores + peak expansion + bonus peaks)\n",
    "Find first point with high z-score.\n",
    "\n",
    "Backtrack point by point, until the diff of this point is suddenly much lower. This is start of peak. Save the Y value of this point.\n",
    "\n",
    "Find last point with high-zscore.\n",
    "\n",
    "Look ahead point by point until Y value comes close to the saved Y value (start of peak). This is done via a percentage of the normalized peak height. (Top of peak minus min value of peak) If the course starts reversing (going down again before reaching previous peak height), also stop the lookahead.\n",
    "\n",
    "To increase recall, points where there is a sudden sharp drop in diff are also marked as peaks, regardless of whether there is a peak in the deseasonalized data or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64801\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99dbc75a8fce442baaae4c750eddd0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17e0619e040>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "\n",
    "# -------- Algorithm Parameters ----------------------- #\n",
    "WINDOW_SIZE = 4333#675            # Number of samples in the window.\n",
    "Z_THRESHOLD = 4.62#5.5            # Minimum Z-Score for point to be marked as peak.\n",
    "\n",
    "MAXIMUM_LOOKAHEAD = 720      # Stop expanding peak after X samples if start/end not found yet.\n",
    "PEAK_START_DIFF_FACTOR = 2.28#4   # Peak start defined as when diff is suddenly at least N times smaller.\n",
    "PEAK_END_RECOVER_PERC = 0.83#0.65 # Peak end defined when N% of the pre-peak value has been regained.\n",
    "\n",
    "BONUS_DIFF_THRES = final_ts.mean() / 15.3#19 # The minimum Diff value for a point to be marked as a bonus peak\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 4784#675            # Number of samples in the window.\n",
    "Z_THRESHOLD = 4.86#5.5            # Minimum Z-Score for point to be marked as peak.\n",
    "\n",
    "MAXIMUM_LOOKAHEAD = 720      # Stop expanding peak after X samples if start/end not found yet.\n",
    "PEAK_START_DIFF_FACTOR = 2.06#4   # Peak start defined as when diff is suddenly at least N times smaller.\n",
    "PEAK_END_RECOVER_PERC = 0.84#0.65 # Peak end defined when N% of the pre-peak value has been regained.\n",
    "\n",
    "BONUS_DIFF_THRES = final_ts.mean() / 15.3#19 # The minimum Diff value for a point to be marked as a bonus peak\n",
    "\n",
    "\n",
    "#[4784, 4.863266543366097, 5757, 2.0560282392446982, 0.8423319799948153]\n",
    "# -------- PeakData dataframe shape definition -------- #\n",
    "peakData = pd.DataFrame(columns=['group', 'timespan', 'magnitude', 'drop', 'shape', 'startdate', 'enddate'])\n",
    "# ----------------------------------------------------- #\n",
    "\n",
    "\n",
    "\n",
    "def zscore(x, window):\n",
    "    r = x.rolling(window=window)\n",
    "    m = r.mean().shift(1)\n",
    "    s = r.std(ddof=0).shift(1)\n",
    "    z = (x-m)/s\n",
    "    return z\n",
    "\n",
    "def label_dataset(window_size, variance, min_slope_neg, min_slope_pos, max_lookahead, start_diff_factor, end_perc):\n",
    "    # Z-score algorithm to find first set of peaks\n",
    "    labels = zscore(deseasonalized,window_size)<-variance\n",
    "    print(len(labels == True))\n",
    "\n",
    "    combined_df = pd.DataFrame(final_ts)\n",
    "    combined_df.columns = [\"value\"] \n",
    "    combined_df.insert(1, 'label', labels)\n",
    "    #combined_df.insert(1, 'label', False)\n",
    "    combined_df.insert(2, 'diff', final_ts.diff())\n",
    "    combined_df.insert(3, 'deseas_diff', deseasonalized.diff())\n",
    "\n",
    "    # Mark bonus peaks\n",
    "    combined_df.loc[combined_df['diff'] < -min_slope_neg, 'label'] = True\n",
    "    combined_df.loc[combined_df['diff'] > min_slope_pos, 'label'] = True\n",
    "\n",
    "    labels = combined_df['label']\n",
    "\n",
    "    # Get individual peak 'groups' from the raw per-sample labels\n",
    "    labels_shifted = labels.shift(1).fillna(False)\n",
    "    is_edge = labels != labels_shifted\n",
    "    edges = pd.DataFrame({'data': labels, 'edge': is_edge})\n",
    "    peakGroups = edges.loc[edges['edge']==True]\n",
    "\n",
    "    # Expand each peak group to capture entire peak\n",
    "    for i, g in peakGroups.groupby(np.arange(len(peakGroups)) // 2):\n",
    "        peakStart = g.index[0]\n",
    "        peakEnd = g.index[1]\n",
    "\n",
    "        realPeakStart = []\n",
    "        realPeakStartValue = 0\n",
    "\n",
    "        previousDiff = 0\n",
    "        n = 0 # limit on how far to look back to find start of peak\n",
    "        currRow = combined_df.loc[peakStart]\n",
    "\n",
    "        while (True and n < max_lookahead):\n",
    "            currDiff = currRow['diff']\n",
    "\n",
    "            if (abs(currDiff)*start_diff_factor < abs(previousDiff)):\n",
    "                # Start of peak found\n",
    "                realPeakStart = currRow\n",
    "                realPeakStartValue = currRow['value']\n",
    "                break\n",
    "\n",
    "            # Check next row\n",
    "            n += 1\n",
    "            previousDiff = currDiff\n",
    "            currRow = combined_df.loc[currRow.name - pd.Timedelta(minutes=RESAMPLE_FREQ)]\n",
    "\n",
    "        peakMinimum = combined_df.loc[realPeakStart.name:peakEnd, 'value'].min()\n",
    "\n",
    "        realPeakEnd = []\n",
    "\n",
    "        n = 0 # limit on how far to look forward to find end of peak\n",
    "        previousVal = 0\n",
    "        currRow = combined_df.loc[peakEnd]\n",
    "        while (True and n < max_lookahead):\n",
    "            currVal = currRow['value']\n",
    "\n",
    "            if (currVal-peakMinimum > end_perc * (realPeakStartValue-peakMinimum) or currVal < previousVal):\n",
    "                # End of peak found\n",
    "                realPeakEnd = currRow\n",
    "                break\n",
    "\n",
    "            # Check next row\n",
    "            n += 1\n",
    "            previousVal = currVal\n",
    "            currRow = combined_df.loc[currRow.name + pd.Timedelta(minutes=RESAMPLE_FREQ)]\n",
    "        \n",
    "        # Set all the sample labels between the real peak start and end to True\n",
    "        combined_df.loc[realPeakStart.name:realPeakEnd.name-pd.Timedelta(minutes=RESAMPLE_FREQ), 'label'] = True\n",
    "\n",
    "        # Add this peak and its statistics to the peakData dataframe.\n",
    "        peakData.loc[i] = pd.Series({'group':i, \n",
    "                'timespan':(realPeakEnd.name - realPeakStart.name),\n",
    "                'magnitude': combined_df.loc[realPeakStart.name:realPeakEnd.name, 'value'].max() - combined_df.loc[realPeakStart.name:realPeakEnd.name, 'value'].min(),\n",
    "                'drop': 100 - (combined_df.loc[realPeakStart.name:realPeakEnd.name, 'value'].min() / combined_df.loc[realPeakStart.name:realPeakEnd.name, 'value'].max() * 100), \n",
    "                'shape': combined_df.loc[realPeakStart.name:realPeakEnd.name, 'diff'].mean(),\n",
    "                'startdate': realPeakStart.name, \n",
    "                'enddate': realPeakEnd.name})\n",
    "\n",
    "    return combined_df['label']\n",
    "\n",
    "\n",
    "\n",
    "# ---- Perform automatic labelling using predefined parameters ----\n",
    "labeled_dataset = label_dataset(WINDOW_SIZE, Z_THRESHOLD, BONUS_DIFF_THRES, BONUS_DIFF_THRES, MAXIMUM_LOOKAHEAD, PEAK_START_DIFF_FACTOR, PEAK_END_RECOVER_PERC)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Save peak data to a CSV file\n",
    "peakData.to_csv(PATH + '_peaks.csv')\n",
    "\n",
    "# Plot results\n",
    "plt.plot(deseasonalized)\n",
    "x = np.where(labeled_dataset == True)[0]\n",
    "plt.plot(final_ts)\n",
    "plt.plot(deseasonalized.index[x], deseasonalized.values[x], 'ro')\n",
    "plt.plot(final_ts.index[x], final_ts.values[x], 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: Find optimal parameters for the labeling algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209646\n",
      "-0.6850802199190285\n",
      "209646\n",
      "-0.6587285061487649\n",
      "209646\n",
      "-0.6954388594289169\n",
      "209646\n",
      "-0.5824562991718014\n",
      "209646\n",
      "-0.7275388483883323\n",
      "209646\n",
      "-0.6924912215021484\n",
      "209646\n",
      "-0.6619068721628225\n",
      "209646\n",
      "-0.6606762264038641\n",
      "209646\n",
      "-0.35091193935985066\n",
      "209646\n",
      "-0.5545102602184173\n",
      "209646\n",
      "-0.11031270292688145\n",
      "209646\n",
      "-0.7129600721994662\n",
      "209646\n",
      "-0.6904753966876928\n",
      "209646\n",
      "-0.7062949274595822\n",
      "209646\n",
      "-0.6893444789759954\n",
      "209646\n",
      "-0.7191818993288465\n",
      "209646\n",
      "-0.6981271467857078\n",
      "209646\n",
      "-0.7191464576469891\n",
      "209646\n",
      "-0.651408132565743\n",
      "209646\n",
      "-0.7090799086286721\n",
      "209646\n",
      "-0.6884127055979279\n",
      "209646\n",
      "-0.7014191278399364\n",
      "209646\n",
      "-0.7151942008880381\n",
      "209646\n",
      "-0.6836784635837387\n",
      "209646\n",
      "-0.7072150362480867\n",
      "209646\n",
      "-0.5679091096864637\n",
      "209646\n",
      "-0.6566636576711888\n",
      "209646\n",
      "-0.6516962596274682\n",
      "209646\n",
      "-0.5681287492420696\n",
      "209646\n",
      "-0.6659240318251691\n",
      "209646\n",
      "-0.34622694801828613\n",
      "209646\n",
      "-0.5525873159492372\n",
      "209646\n",
      "-0.5739737310733352\n",
      "209646\n",
      "-0.6297813909945507\n",
      "209646\n",
      "-0.5913850390154196\n",
      "209646\n",
      "-0.56582785958823\n",
      "209646\n",
      "-0.4783147739943743\n",
      "209646\n",
      "-0.654076481943727\n",
      "209646\n",
      "-0.6873685531836579\n",
      "209646\n",
      "-0.7143718716063033\n",
      "209646\n",
      "-0.6938070167702823\n",
      "209646\n",
      "-0.7102174988291219\n",
      "209646\n",
      "-0.5084182799335264\n",
      "209646\n",
      "-0.7049876292544703\n",
      "209646\n",
      "-0.6779310731707033\n",
      "209646\n",
      "-0.7315379021835748\n",
      "209646\n",
      "-0.7267173932930424\n",
      "209646\n",
      "-0.6059186294139941\n",
      "209646\n",
      "-0.7092063619041427\n",
      "209646\n",
      "-0.5093971480930959\n",
      "209646\n",
      "-0.6175310203890215\n",
      "209646\n",
      "-0.6303283843382067\n",
      "209646\n",
      "-0.5305118199537694\n",
      "209646\n",
      "-0.7165032057041262\n",
      "209646\n",
      "-0.7160170383005318\n",
      "209646\n",
      "-0.686684177428157\n",
      "209646\n",
      "-0.6910165788306557\n",
      "209646\n",
      "-0.6951400793114421\n",
      "209646\n",
      "-0.6885347340777812\n",
      "209646\n",
      "-0.680983676563017\n",
      "209646\n",
      "-0.22810780114458906\n",
      "209646\n",
      "-0.6868772898262294\n",
      "209646\n",
      "-0.6722549958636176\n",
      "209646\n",
      "-0.6867838764895529\n",
      "209646\n",
      "-0.6090885581189527\n",
      "209646\n",
      "-0.7238208921538202\n",
      "209646\n",
      "-0.6628239834401136\n",
      "209646\n",
      "-0.6655421820217827\n",
      "209646\n",
      "-0.6306269658537054\n",
      "209646\n",
      "-0.723028541446634\n",
      "209646\n",
      "-0.6697176200752333\n",
      "209646\n",
      "-0.5271497881449978\n",
      "209646\n",
      "-0.7280508587588724\n",
      "209646\n",
      "-0.746862278847788\n",
      "209646\n",
      "-0.6833754212305405\n",
      "209646\n",
      "-0.7432879093007823\n",
      "209646\n",
      "-0.7322251153679035\n",
      "209646\n",
      "-0.7067834650766458\n",
      "209646\n",
      "-0.7076040360730191\n",
      "209646\n",
      "-0.7262206757203091\n",
      "209646\n",
      "-0.7322100940452317\n",
      "209646\n",
      "-0.7340261853200003\n",
      "209646\n",
      "-0.7301680562370678\n",
      "209646\n",
      "-0.6731319584695571\n",
      "209646\n",
      "-0.6915793856598009\n",
      "209646\n",
      "-0.6649400757568595\n",
      "209646\n",
      "-0.709322113015981\n",
      "209646\n",
      "-0.6842575325752212\n",
      "209646\n",
      "-0.5228258590361057\n",
      "209646\n",
      "-0.7607567035206347\n",
      "209646\n",
      "-0.7676512743257979\n",
      "209646\n",
      "-0.7625035432073947\n",
      "209646\n",
      "-0.7418638334494311\n",
      "209646\n",
      "-0.7243574238450678\n",
      "209646\n",
      "-0.6904505021839672\n",
      "209646\n",
      "-0.7203762918675798\n",
      "209646\n",
      "-0.7408551785787139\n",
      "209646\n",
      "-0.5169548905674695\n",
      "209646\n",
      "-0.7210951649482507\n",
      "209646\n",
      "-0.6967597913114277\n",
      "best result:  0.7676512743257979\n",
      "best parameters:  [4784, 4.863266543366097, 5757, 2.0560282392446982, 0.8423319799948153]\n"
     ]
    }
   ],
   "source": [
    "import skopt\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "\n",
    "def objective(search_params):\n",
    "    df_real = pd.read_csv(REAL_LABELS_FILE)\n",
    "    df_pred = label_dataset(search_params[0], search_params[1], search_params[2], search_params[2], MAXIMUM_LOOKAHEAD, search_params[3], search_params[4]).astype(int)\n",
    "    #tn, fp, fn, tp = confusion_matrix(df_real['label'], df_pred).ravel()\n",
    "    #f1_score = (tp)/(tp+0.5*(fp+fn))\n",
    "    #return -1* f1_score\n",
    "    score = -1* cohen_kappa_score(df_real['label'], df_pred)\n",
    "    print(score)\n",
    "    return score\n",
    "\n",
    "SPACE = [\n",
    "    skopt.space.Integer(3000, 8000, name='window_size'),\n",
    "    skopt.space.Real(3, 6.5, name='variability_thres', prior='uniform'),\n",
    "    skopt.space.Integer(800, 6000, name='bonus_diff_threshold'),\n",
    "    skopt.space.Real(2, 6, name='start_diff', prior='uniform'),\n",
    "    skopt.space.Real(0.45, 0.85, name='recovery_perc', prior='uniform')]\n",
    "\n",
    "def run_optimizer():\n",
    "    results = skopt.forest_minimize(objective, SPACE, n_calls=100, n_random_starts=10)\n",
    "    #results = skopt.BayesSearchCV(objective, SPACE, n_calls=300, n_random_starts=10)\n",
    "    best_auc = -1.0 * results.fun\n",
    "    best_params = results.x\n",
    "    print('best result: ', best_auc)\n",
    "    print('best parameters: ', best_params)\n",
    "\n",
    "#run_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result of the automatic labelling\n",
    "Shows a confusion matrix and the F-score for the previously performed automatic labelling.\n",
    "Compared against a manually labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7724631202503353\n",
      "0.7712466124406792\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91f053a50f74bfe9ab5675e0e61d4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "df_real = pd.read_csv(REAL_LABELS_FILE)\n",
    "df_pred = labeled_dataset.astype(int)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(df_real['label'].astype(int), df_pred).ravel()\n",
    "f1_score = (tp)/(tp+0.5*(fp+fn))\n",
    "print(f1_score)\n",
    "print(cohen_kappa_score(df_real['label'], df_pred))\n",
    "\n",
    "cm = confusion_matrix(df_real['label'].astype(int), df_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"no peak\", \"peak\"])\n",
    "disp = disp.plot(include_values=[\"no peak\", \"peak\"], values_format=\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c849083521eda5a65ad42de022766a29d6e707347fcbb28c1498394b4ac1ebb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
